{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanivZimmer/collision/blob/main/fe_frame_and_sequence_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVftjkU9BuBQ",
        "outputId": "4c13322e-1b17-489a-a66d-83fac9f84daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample_submission.csv',\n",
              " 'test.csv',\n",
              " 'train.csv',\n",
              " '.DS_Store',\n",
              " 'test',\n",
              " 'train',\n",
              " 'weights',\n",
              " 'submission.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "root_data = \"hide/nexar-collision-prediction\"\n",
        "os.listdir(root_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "SjKcVtBdBHYi",
        "outputId": "fda220ba-7261-460b-d3c6-ee8d0ee9312e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 209MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "  0%|          | 0/47 [00:15<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-29c4c18455ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-29c4c18455ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, fe, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mframe_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mframe_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-29c4c18455ff>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{video_id}.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-29c4c18455ff>\u001b[0m in \u001b[0;36mload_video\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "# Dataset Class\n",
        "class AccidentDataset(Dataset):\n",
        "    def __init__(self, csv_file, video_dir, feature_extractor,frames_per_clip=16, max_frames=30*45): # 30 in sec, ~40 se per\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.video_dir = video_dir\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        #self.max_frames = max_frames\n",
        "\n",
        "        self.feature_extractor = feature_extractor#models.mobilenet_v2(pretrained=True)\n",
        "        self.feature_extractor.classifier = torch.nn.Identity()\n",
        "        #self.feature_extractor.eval()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = str(int(self.data.iloc[idx]['id'])).zfill(5)\n",
        "        label = torch.tensor(self.data.iloc[idx]['target'], dtype=torch.float32)\n",
        "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        frames = self.load_video(video_path)\n",
        "        features = self.extract_features(frames)\n",
        "\n",
        "        return features, label, len(features)\n",
        "\n",
        "    def load_video(self, video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total_frames == 0:\n",
        "            cap.release()\n",
        "            return frames\n",
        "\n",
        "        frame_idxs = torch.linspace(0, total_frames - 1, self.frames_per_clip).long().tolist()\n",
        "        for i in range(total_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if i in frame_idxs:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "                frames.append(Image.fromarray(frame))\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def extract_features(self, frames):\n",
        "        features = []\n",
        "        for frame in frames:\n",
        "            frame = self.transform(frame).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                feature = self.feature_extractor(frame).squeeze(0)\n",
        "            features.append(feature)\n",
        "\n",
        "        return torch.stack(features) if features else torch.zeros(1, 1280)\n",
        "\n",
        "# Collate Function\n",
        "def collate_fn(batch):\n",
        "    features, labels, lengths = zip(*batch)\n",
        "    max_length = max(lengths)\n",
        "    padded_features = torch.zeros(len(batch), max_length, 1280)\n",
        "    mask = torch.ones(len(batch), max_length, dtype=torch.bool)\n",
        "\n",
        "    for i, (feat, length) in enumerate(zip(features, lengths)):\n",
        "        padded_features[i, :length, :] = feat\n",
        "        mask[i, :length] = False\n",
        "\n",
        "    return padded_features, torch.tensor(labels), mask\n",
        "\n",
        "# Transformer Model\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim=1280, num_heads=8, num_layers=4):\n",
        "        super(VideoTransformer, self).__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 100, feature_dim))\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(feature_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, frame_features, mask):\n",
        "        seq_length = frame_features.size(1)\n",
        "        frame_features += self.pos_embedding[:, :seq_length, :]\n",
        "        transformed_features = self.transformer(frame_features, src_key_padding_mask=mask.T)\n",
        "        output = self.fc(transformed_features.mean(dim=1))\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "# Training Function\n",
        "def train(model,fe, dataloader, epochs=10, lr=1e-4):\n",
        "    model.train()\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {'params': fe.parameters(), 'lr': 1e-5},  # Smaller LR for MobileNet\n",
        "        {'params': model.parameters(), 'lr': lr}  # Larger LR for Transformer\n",
        "    ])\n",
        "\n",
        "    criterion = nn.BCELoss()#nn.MSELoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for frame_features, labels, mask in tqdm(dataloader):\n",
        "            frame_features, labels, mask = frame_features.to(device), labels.to(device), mask.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(frame_features, mask)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n",
        "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/Data/vit_{epoch+1}.pth\")\n",
        "        torch.save(fe.state_dict(), f\"/content/drive/MyDrive/Data/mobile_{epoch+1}.pth\")\n",
        "\n",
        "\n",
        "\n",
        "# Load Dataset and Start Training\n",
        "fe=models.mobilenet_v2(pretrained=True)\n",
        "fe.classifier = nn.Identity()  # Remove final classification layer\n",
        "\n",
        "# Fine-tuning strategy: Freeze early layers, train later layers\n",
        "for param in fe.features[:10].parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in fe.features[10:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "fe.train()  # Set MobileNet to training mode\n",
        "dataset = AccidentDataset(csv_file=f\"{root_data}/train.csv\", video_dir=f\"{root_data}/train\",feature_extractor=fe)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "model = VideoTransformer()\n",
        "train(model,fe, dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "\n",
        "# Load DINOv2 model and processor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fe = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device).eval()\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/dinov2-base\",use_fast=True)\n",
        "\n",
        "# Dataset Class\n",
        "class AccidentDataset(Dataset):\n",
        "    def __init__(self, csv_file, video_dir, feature_extractor, processor, frames_per_clip=16):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.video_dir = video_dir\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = str(int(self.data.iloc[idx]['id'])).zfill(5)\n",
        "        label = torch.tensor(self.data.iloc[idx]['target'], dtype=torch.float32)\n",
        "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        frames = self.load_video(video_path)\n",
        "        features = self.extract_features(frames)\n",
        "\n",
        "        return features, label, len(features)\n",
        "\n",
        "    def load_video(self, video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total_frames == 0:\n",
        "            cap.release()\n",
        "            return frames\n",
        "\n",
        "        frame_idxs = torch.linspace(0, total_frames - 1, self.frames_per_clip).long().tolist()\n",
        "        for i in range(total_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if i in frame_idxs:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(Image.fromarray(frame))\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def extract_features(self, frames):\n",
        "        features = []\n",
        "        for frame in frames:\n",
        "            inputs = self.processor(images=frame, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                feature = self.feature_extractor(**inputs).last_hidden_state.mean(dim=1).squeeze(0)\n",
        "            features.append(feature)\n",
        "\n",
        "        return torch.stack(features) if features else torch.zeros(1, 768)  # DINOv2 base has 768-dim features\n",
        "\n",
        "# Collate Function\n",
        "def collate_fn(batch):\n",
        "    features, labels, lengths = zip(*batch)\n",
        "    max_length = max(lengths)\n",
        "    padded_features = torch.zeros(len(batch), max_length, 768, device=device)  # Adjusted for DINOv2's 768-dim output\n",
        "    mask = torch.ones(len(batch), max_length, dtype=torch.bool, device=device)\n",
        "\n",
        "    for i, (feat, length) in enumerate(zip(features, lengths)):\n",
        "        padded_features[i, :length, :] = feat\n",
        "        mask[i, :length] = False\n",
        "\n",
        "    return padded_features, torch.tensor(labels, device=device), mask\n",
        "\n",
        "# Transformer Model\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim=768, num_heads=8, num_layers=4):\n",
        "        super(VideoTransformer, self).__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 100, feature_dim, device=device))\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(feature_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, frame_features, mask):\n",
        "        seq_length = frame_features.size(1)\n",
        "        frame_features += self.pos_embedding[:, :seq_length, :]\n",
        "        transformed_features = self.transformer(frame_features, src_key_padding_mask=mask.T)\n",
        "        output = self.fc(transformed_features.mean(dim=1))\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "# Training Function\n",
        "def train(model, fe, dataloader, epochs=10, lr=1e-4):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {'params': model.parameters(), 'lr': lr}  # Fine-tuning only Transformer\n",
        "    ])\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for frame_features, labels, mask in tqdm(dataloader):\n",
        "            frame_features, labels, mask = frame_features.to(device), labels.to(device), mask.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(frame_features, mask)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n",
        "        torch.save(model.state_dict(), f\"vit_{epoch+1}.pth\")\n",
        "\n",
        "# Load Dataset and Start Training\n",
        "dataset = AccidentDataset(csv_file=f\"{root_data}/train.csv\", video_dir=f\"{root_data}/train\",\n",
        "                          feature_extractor=fe, processor=processor)\n",
        "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "model = VideoTransformer()\n",
        "train(model, fe, dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419,
          "referenced_widgets": [
            "4f1d744f5a564caa8f359b809ddb04e2",
            "f0fbc4c8dd784c408277d7ab7b5f9f82",
            "fba504ae05be436d9b8a90170698c340",
            "bd2a5512c57548b5ac87df56f9e8cef8",
            "a2cc6383afe64a9f95dd6e8cd9caa83f",
            "0cd7933a43684250a162d4d0657cfe18",
            "2d412aa09cde4fe49d76624040cb5138",
            "3395753b6f3346e4adbe69ac84eaa18d",
            "2f176c21790f4f248e463741e8840157",
            "b2e60d6c2b9544c9b14b0396d00a4bac",
            "e5f5883be44d400fbc7af25b24518200",
            "8b48b05628ca49d38c2197ba3c306d54",
            "93d6199ae5144836abeb32552a8b59f7",
            "2c9f2dc11af341a28b78f93eb03ed634",
            "66675ae3d196449eb540b7f894040df1",
            "dd7f0a51e5af47d2acaea6b3197151eb",
            "4de741a745214d4f950d5e66915db63e",
            "d56d73e4e8b2479fad6380d39fc8f05e",
            "fd801b779acf4cc3a253d9e5600a1e12",
            "6a2383b165e54723b23dca1adbe3fbf0",
            "94835a1c871b4ee2a7977d05566bfed8",
            "003af6a274d0487db7bc5fd159a0b8a0",
            "af7e0b39993344768a6f3ed855d98fa3",
            "f25cd58a5bf743f4924c76b47942e753",
            "8ecb2e7332594829a0ddab8cfbabf480",
            "374e415ef05344c28ea83c02b6963398",
            "e9bd1ef0d8e14fb8895144614ecc0d17",
            "680512235fd24ac3a2efee63977cd417",
            "b550207da01543aca443bcff1136a301",
            "f6780e86af8d492d8de8227a80b777e6",
            "a2424f9c44294bd4a2f47242f5eb473a",
            "33a55e75b4434fdfb2906218e0a38214",
            "487f4761977e4639910d4bddd24c2457"
          ]
        },
        "id": "Um5ljqAft6fq",
        "outputId": "47773a34-6d68-4903-808d-b15c9063bef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f1d744f5a564caa8f359b809ddb04e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b48b05628ca49d38c2197ba3c306d54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af7e0b39993344768a6f3ed855d98fa3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "100%|██████████| 94/94 [1:25:41<00:00, 54.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [1:10:48<00:00, 45.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.5941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [1:09:33<00:00, 44.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.5890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 2/94 [01:12<56:24, 36.79s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq-Xzk1qAkh3"
      },
      "outputs": [],
      "source": [
        "class BlindAccidentDataset(AccidentDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = str(int(self.data.iloc[idx]['id'])).zfill(5)\n",
        "\n",
        "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        frames = self.load_video(video_path)\n",
        "\n",
        "        inputs = self.feature_extractor(frames, return_tensors=\"pt\")\n",
        "        return inputs['pixel_values'].squeeze(0) # Return only pixel values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAjfeZ5sBBAD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Create a prediction function\n",
        "def predict_on_test(model, dataloader):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs in tqdm(dataloader):  # No labels for test set\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(pixel_values=inputs).logits\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "    return predictions\n",
        "\n",
        "# Load the test dataset and create dataloader\n",
        "test_csv = os.path.join(root_data, \"test.csv\")\n",
        "test_video_dir = os.path.join(root_data, \"test\")\n",
        "\n",
        "testset = BlindAccidentDataset(test_csv, test_video_dir, fe)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)  # Important: shuffle=False\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = predict_on_test(model, testloader)\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(test_csv)\n",
        "\n",
        "# for i, pred in enumerate(test_predictions):\n",
        "#   print(f\"Video {test_df.iloc[i]['id']}: Prediction Probability = {pred[0]:.4f}\")\n",
        "\n",
        "\n",
        "#save them to a CSV\n",
        "submission_df = pd.DataFrame({'id': test_df['id'], 'target': [p[0] for p in test_predictions]})\n",
        "submission_df.to_csv('hide/submission_fe_and_transformers.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load video on dataset\n",
        "is not efficient. improve it as its bottleneck no matter the model\\gpu"
      ],
      "metadata": {
        "id": "yj9-DphbUeU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6_TBvfAXJrN",
        "outputId": "7c0c631d-cfa6-49d9-e10c-f85ce5fe33d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (1.26.4)\n",
            "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: decord\n",
            "Successfully installed decord-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel\n",
        "\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "decord.bridge.set_bridge('torch')  # Use PyTorch-native tensor output\n",
        "\n",
        "# Load DINOv2 model\n",
        "fe = AutoModel.from_pretrained(\"facebook/dinov2-base\").eval()\n",
        "\n",
        "# Dataset Class\n",
        "class AccidentDataset(Dataset):\n",
        "    def __init__(self, csv_file, video_dir, feature_extractor, frames_per_clip=16):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.video_dir = video_dir\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = str(int(self.data.iloc[idx]['id'])).zfill(5)\n",
        "        label = torch.tensor(self.data.iloc[idx]['target'], dtype=torch.float32)\n",
        "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        frames = self.load_video(video_path)\n",
        "        features = self.extract_features(frames)\n",
        "\n",
        "        return features, label, len(features)\n",
        "\n",
        "\n",
        "    def load_videocv(self, video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        if total_frames == 0 or not cap.isOpened():\n",
        "            cap.release()\n",
        "            return frames\n",
        "\n",
        "        frame_idxs = torch.linspace(0, total_frames - 1, self.frames_per_clip).long().tolist()\n",
        "\n",
        "        for idx in frame_idxs:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # Seek to frame\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, (224, 224))\n",
        "            frames.append(Image.fromarray(frame))\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def load_video_dec(self, video_path):\n",
        "        decord.bridge.set_bridge('torch')  # Enable PyTorch tensor output\n",
        "        vr = VideoReader(video_path, ctx=cpu(0))  # Load video with CPU decoding\n",
        "        total_frames = len(vr)\n",
        "\n",
        "        if total_frames == 0:\n",
        "            return []\n",
        "\n",
        "        frame_idxs = torch.linspace(0, total_frames - 1, self.frames_per_clip).long()\n",
        "\n",
        "        # Efficiently load selected frames\n",
        "        frames = vr.get_batch(frame_idxs).byte().permute(0, 2, 3, 1).numpy()  # Convert to (H, W, C)\n",
        "\n",
        "        # Ensure frames are RGB (3 channels)\n",
        "        if frames.shape[-1] == 1:  # If grayscale, convert to RGB\n",
        "            frames = np.repeat(frames, 3, axis=-1)\n",
        "\n",
        "        # Convert frames to PIL images\n",
        "        frames = [Image.fromarray(frame, mode=\"RGB\") for frame in frames]\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def load_video(self, video_path, frames_per_clip=16, device=\"cuda\"):\n",
        "        vr = VideoReader(video_path)  # Load video\n",
        "        total_frames = len(vr)\n",
        "\n",
        "        if total_frames == 0:\n",
        "            print(f\"Warning: {video_path} has no frames.\")\n",
        "            return torch.empty(0, device=device)  # Return an empty tensor on the correct device\n",
        "\n",
        "        frame_idxs = torch.linspace(0, total_frames - 1, frames_per_clip, dtype=torch.int64)\n",
        "\n",
        "        # Load frames efficiently and normalize in-place\n",
        "        frames = vr.get_batch(frame_idxs).permute(0, 3, 1, 2).to(torch.float32)  # (T, H, W, C) -> (T, C, H, W)\n",
        "        frames.div_(255.0)  # In-place normalization to [0, 1]\n",
        "\n",
        "        return frames.to(device, non_blocking=True)  # Move to GPU if needed\n",
        "\n",
        "\n",
        "    def extract_features(self, frames):\n",
        "        device = next(self.feature_extractor.parameters()).device  # Get model's device\n",
        "        features = []\n",
        "\n",
        "        for frame in frames:\n",
        "            frame = self.transform(frame).unsqueeze(0).to(device)  # Move frame to model's device\n",
        "            with torch.no_grad():\n",
        "                feature = self.feature_extractor(frame).last_hidden_state.mean(dim=1).squeeze(0)\n",
        "            features.append(feature)\n",
        "\n",
        "        return torch.stack(features) if features else torch.zeros(1, 768, device=device)  # Ensure consistency\n",
        "\n",
        "    def extract_features2(self, frames):\n",
        "        features = []\n",
        "        for frame in frames:\n",
        "            frame = self.transform(frame).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                feature = self.feature_extractor(frame).last_hidden_state.mean(dim=1).squeeze(0)\n",
        "            features.append(feature)\n",
        "\n",
        "        return torch.stack(features) if features else torch.zeros(1, 768)\n",
        "\n",
        "# Collate Function\n",
        "def collate_fn(batch):\n",
        "    features, labels, lengths = zip(*batch)\n",
        "    max_length = max(lengths)\n",
        "    padded_features = torch.zeros(len(batch), max_length, 768)\n",
        "    mask = torch.ones(len(batch), max_length, dtype=torch.bool)\n",
        "\n",
        "    for i, (feat, length) in enumerate(zip(features, lengths)):\n",
        "        padded_features[i, :length, :] = feat\n",
        "        mask[i, :length] = False\n",
        "\n",
        "    return padded_features, torch.tensor(labels), mask\n",
        "\n",
        "# Bi-LSTM Model\n",
        "class VideoBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=512, num_layers=2):\n",
        "        super(VideoBiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)  # Bi-directional, so 2x hidden_dim\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, frame_features, mask):\n",
        "        packed_output, _ = self.lstm(frame_features)\n",
        "        output = self.fc(packed_output[:, -1, :])  # Take the last output for classification\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "# Training Function\n",
        "def train(model, fe, dataloader, epochs=10, lr=1e-4):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam([\n",
        "        #{'params': fe.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.parameters(), 'lr': lr}\n",
        "    ])\n",
        "    criterion = nn.BCELoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    fe.to(device)\n",
        "    fe.eval()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for frame_features, labels, mask in tqdm(dataloader):\n",
        "            frame_features, labels, mask = frame_features.to(device), labels.to(device), mask.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(frame_features, mask)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n",
        "        torch.save(model.state_dict(), f\"bilstm_{epoch+1}.pth\")\n",
        "        torch.save(fe.state_dict(), f\"dinov2_{epoch+1}.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6tLCGp5uGUwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UUIYwAhTopyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load model from saved\n",
        "\n",
        "# Load the trained model (replace with your actual model path)\n",
        "model = VideoBiLSTM()\n",
        "\n",
        "# model_path = \"bilstm_1.pth\" #@param {type:\"string\"}\n",
        "# model.load_state_dict(torch.load(model_path))\n",
        "# model.to(device)\n",
        "# model.eval()\n"
      ],
      "metadata": {
        "id": "l9SCMNw-S-HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the source and destination directories\n",
        "source_directory = f\"{root_data}\"\n",
        "destination_directory = \"/content/destination_directory\"\n",
        "\n",
        "# Copy the entire directory\n",
        "shutil.copytree(source_directory, destination_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mC6iTCO3p74j",
        "outputId": "a8a9adf2-18b0-4995-d074-6e69a40221d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/destination_directory'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset and Start Training\n",
        "dataset = AccidentDataset(csv_file=f\"/content/destination_directory/train.csv\", video_dir=f\"/content/destination_directory/train\", feature_extractor=fe)\n",
        "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "train(model, fe, dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E4ZEUZZS6KN",
        "outputId": "455cbff9-c0df-411e-ef13-7a2607fa56a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 40/94 [50:07<1:05:56, 73.27s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOKW3MrJvkRCLzZXeumjT+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f1d744f5a564caa8f359b809ddb04e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0fbc4c8dd784c408277d7ab7b5f9f82",
              "IPY_MODEL_fba504ae05be436d9b8a90170698c340",
              "IPY_MODEL_bd2a5512c57548b5ac87df56f9e8cef8"
            ],
            "layout": "IPY_MODEL_a2cc6383afe64a9f95dd6e8cd9caa83f"
          }
        },
        "f0fbc4c8dd784c408277d7ab7b5f9f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cd7933a43684250a162d4d0657cfe18",
            "placeholder": "​",
            "style": "IPY_MODEL_2d412aa09cde4fe49d76624040cb5138",
            "value": "config.json: 100%"
          }
        },
        "fba504ae05be436d9b8a90170698c340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3395753b6f3346e4adbe69ac84eaa18d",
            "max": 548,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f176c21790f4f248e463741e8840157",
            "value": 548
          }
        },
        "bd2a5512c57548b5ac87df56f9e8cef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2e60d6c2b9544c9b14b0396d00a4bac",
            "placeholder": "​",
            "style": "IPY_MODEL_e5f5883be44d400fbc7af25b24518200",
            "value": " 548/548 [00:00&lt;00:00, 60.5kB/s]"
          }
        },
        "a2cc6383afe64a9f95dd6e8cd9caa83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd7933a43684250a162d4d0657cfe18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d412aa09cde4fe49d76624040cb5138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3395753b6f3346e4adbe69ac84eaa18d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f176c21790f4f248e463741e8840157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2e60d6c2b9544c9b14b0396d00a4bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f5883be44d400fbc7af25b24518200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b48b05628ca49d38c2197ba3c306d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93d6199ae5144836abeb32552a8b59f7",
              "IPY_MODEL_2c9f2dc11af341a28b78f93eb03ed634",
              "IPY_MODEL_66675ae3d196449eb540b7f894040df1"
            ],
            "layout": "IPY_MODEL_dd7f0a51e5af47d2acaea6b3197151eb"
          }
        },
        "93d6199ae5144836abeb32552a8b59f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4de741a745214d4f950d5e66915db63e",
            "placeholder": "​",
            "style": "IPY_MODEL_d56d73e4e8b2479fad6380d39fc8f05e",
            "value": "model.safetensors: 100%"
          }
        },
        "2c9f2dc11af341a28b78f93eb03ed634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd801b779acf4cc3a253d9e5600a1e12",
            "max": 346345912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a2383b165e54723b23dca1adbe3fbf0",
            "value": 346345912
          }
        },
        "66675ae3d196449eb540b7f894040df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94835a1c871b4ee2a7977d05566bfed8",
            "placeholder": "​",
            "style": "IPY_MODEL_003af6a274d0487db7bc5fd159a0b8a0",
            "value": " 346M/346M [00:14&lt;00:00, 23.7MB/s]"
          }
        },
        "dd7f0a51e5af47d2acaea6b3197151eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de741a745214d4f950d5e66915db63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56d73e4e8b2479fad6380d39fc8f05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd801b779acf4cc3a253d9e5600a1e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a2383b165e54723b23dca1adbe3fbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94835a1c871b4ee2a7977d05566bfed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "003af6a274d0487db7bc5fd159a0b8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af7e0b39993344768a6f3ed855d98fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f25cd58a5bf743f4924c76b47942e753",
              "IPY_MODEL_8ecb2e7332594829a0ddab8cfbabf480",
              "IPY_MODEL_374e415ef05344c28ea83c02b6963398"
            ],
            "layout": "IPY_MODEL_e9bd1ef0d8e14fb8895144614ecc0d17"
          }
        },
        "f25cd58a5bf743f4924c76b47942e753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680512235fd24ac3a2efee63977cd417",
            "placeholder": "​",
            "style": "IPY_MODEL_b550207da01543aca443bcff1136a301",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "8ecb2e7332594829a0ddab8cfbabf480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6780e86af8d492d8de8227a80b777e6",
            "max": 436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2424f9c44294bd4a2f47242f5eb473a",
            "value": 436
          }
        },
        "374e415ef05344c28ea83c02b6963398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33a55e75b4434fdfb2906218e0a38214",
            "placeholder": "​",
            "style": "IPY_MODEL_487f4761977e4639910d4bddd24c2457",
            "value": " 436/436 [00:00&lt;00:00, 55.1kB/s]"
          }
        },
        "e9bd1ef0d8e14fb8895144614ecc0d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680512235fd24ac3a2efee63977cd417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b550207da01543aca443bcff1136a301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6780e86af8d492d8de8227a80b777e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2424f9c44294bd4a2f47242f5eb473a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33a55e75b4434fdfb2906218e0a38214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "487f4761977e4639910d4bddd24c2457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}