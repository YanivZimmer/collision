{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanivZimmer/collision/blob/main/transformers_doc/en/video_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgNM0T44zQ93",
        "outputId": "e1c8689d-b0cd-4f34-a1c7-45d815dd3d73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrChY2qFzQ95"
      },
      "source": [
        "# Video classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqKI3mEvzQ96"
      },
      "source": [
        "Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to. These models can be used to categorize what a video is all about. A real-world application of video classification is action / activity recognition, which is useful for fitness applications. It is also helpful for vision-impaired individuals, especially when they are commuting.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Fine-tune [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) on a subset of the [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) dataset.\n",
        "2. Use your fine-tuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "The task illustrated in this tutorial is supported by the following model architectures:\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[TimeSformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/timesformer), [VideoMAE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/videomae)\n",
        "\n",
        "<!--End of the generated tip-->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install -q pytorchvideo transformers evaluate\n",
        "```\n",
        "\n",
        "You will use [PyTorchVideo](https://pytorchvideo.org/) (dubbed `pytorchvideo`) to process and prepare the videos.\n",
        "\n",
        "We encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99mKpx_ik_MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q pytorchvideo transformers evaluate torchvision\n"
      ],
      "metadata": {
        "id": "C5yUqHCGkyBE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pytorchvideo\n"
      ],
      "metadata": {
        "id": "ANO5wO0nmT61",
        "outputId": "622e1722-d732-4f55-e183-2008b96d8281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorchvideo in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (0.1.5.post20221221)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (14.1.0)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch torchvision\n"
      ],
      "metadata": {
        "id": "I8iOKJxvmZ7e",
        "outputId": "9c41c6bf-d8a1-43fa-fc07-26421e58ec87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "Installing collected packages: triton, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "6153859c763f4fb2a8692cdc1eb8da99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    Normalize,\n",
        "    RandomShortSideScale,\n",
        "    RemoveKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        ")\n"
      ],
      "metadata": {
        "id": "A3PpQ85bmhTn",
        "outputId": "f3481f5e-b879-45e6-9c45-3cedc3ceff18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.transforms.functional_tensor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-94ea68c15279>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pytorchvideo.transforms import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mApplyTransformToKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mNormalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRandomShortSideScale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mRemoveKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAugMix\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCutMix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixVideo\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrand_augment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pytorchvideo.transforms.augmentations import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_AUGMENTATION_MAX_LEVEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mAugmentTransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmentations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NbBd1w_mdLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D2XE59AgzQ96",
        "outputId": "15401e2f-497c-4ad2-8d37-a4549d63eb51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "441fc9705f6b4c739d95c57c7d8d0208",
            "e7d20c6afd0248b2956f6bbca1955e83",
            "ef52f039117449709dc6c1d020006588",
            "3bf6e549c52f498da472d0788281fd86",
            "84f8c22c7c444ec2948d8fe4894dc689",
            "5f6ff365fcd24d338137aca8679e5aa4",
            "251bee1aaed14ceb89bfc0840848fa6e",
            "1c6c994e22364c9fa6640baf8688be55",
            "ec99d6398f414a7ab769e0a663240464",
            "83b4f30015284347a82ab9ea6c5ea286",
            "018dd2c59eee449ea1eb0bf0c849bf89",
            "4e3066af95d74634abd9cd03b6584715",
            "da2decdef76b4773b6ecfe061ad5b54a",
            "10ac724140c34267aa947b97be1f8c15",
            "2236c28ebc8c4aacb8e64b7a199c817e",
            "2133b7f105f843fd926e4a419fb2a825",
            "8e01cfe9eb104fcfa1e19121c392db88"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "441fc9705f6b4c739d95c57c7d8d0208"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s44n8CdezQ96"
      },
      "source": [
        "## Load UCF101 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwxNlfgNzQ96"
      },
      "source": [
        "Start by loading a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). This will give you a chance to experiment and make sure everything works before spending more time training on the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l8lzevdjzQ97",
        "outputId": "47b4f6ed-8bed-4e97-c927-8bc709ba8753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
        "filename = \"UCF101_subset.tar.gz\"\n",
        "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWCPsVvdzQ97"
      },
      "source": [
        "After the subset has been downloaded, you need to extract the compressed archive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-fTjMuCyzQ97"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "with tarfile.open(file_path) as t:\n",
        "     t.extractall(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPL4oimAzQ97"
      },
      "source": [
        "At a high level, the dataset is organized like so:\n",
        "\n",
        "```bash\n",
        "UCF101_subset/\n",
        "    train/\n",
        "        BandMarching/\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        Archery\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        ...\n",
        "    val/\n",
        "        BandMarching/\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        Archery\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        ...\n",
        "    test/\n",
        "        BandMarching/\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        Archery\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        ...\n",
        "```\n",
        "\n",
        "The (`sorted`) video paths appear like so:\n",
        "\n",
        "```bash\n",
        "...\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n",
        "...\n",
        "```\n",
        "\n",
        "You will notice that there are video clips belonging to the same group / scene where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi` and `v_ApplyEyeMakeup_g07_c06.avi`, for example.\n",
        "\n",
        "For the validation and evaluation splits, you wouldn't want to have video clips from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage). The subset that you are using in this tutorial takes this information into account.\n",
        "\n",
        "Next up, you will derive the set of labels present in the dataset. Also, create two dictionaries that'll be helpful when initializing the model:\n",
        "\n",
        "* `label2id`: maps the class names to integers.\n",
        "* `id2label`: maps the integers to class names."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: all_video_file_paths is all paths in the UCF101_subset/\n",
        "#     train/\n",
        "# paths defined above\n",
        "\n",
        "import os\n",
        "\n",
        "def get_video_files(root_dir):\n",
        "    \"\"\"Counts the number of video files in a directory.\"\"\"\n",
        "    video_extensions = ('.mp4', '.avi')  # Add other video extensions if needed\n",
        "    count = 0\n",
        "    videos=[]\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(video_extensions):\n",
        "                count += 1\n",
        "                videos.append(os.path.join(dirpath, filename))\n",
        "    return videos\n",
        "\n",
        "# Example usage: Assuming 'UCF101_subset/train' is the directory\n",
        "train_dir = 'UCF101_subset/train'\n",
        "all_video_file_paths = get_video_files(train_dir)\n",
        "print(f\"Number of video files in '{train_dir}': {all_video_file_paths}\")\n"
      ],
      "metadata": {
        "id": "UKBk6_Dj3d4_",
        "outputId": "da81d33e-934a-4466-eb62-13e0a8bcaa20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of video files in 'UCF101_subset/train': ['UCF101_subset/train/Archery/v_Archery_g15_c02.avi', 'UCF101_subset/train/Archery/v_Archery_g24_c06.avi', 'UCF101_subset/train/Archery/v_Archery_g22_c02.avi', 'UCF101_subset/train/Archery/v_Archery_g23_c05.avi', 'UCF101_subset/train/Archery/v_Archery_g11_c06.avi', 'UCF101_subset/train/Archery/v_Archery_g10_c03.avi', 'UCF101_subset/train/Archery/v_Archery_g09_c02.avi', 'UCF101_subset/train/Archery/v_Archery_g11_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g02_c05.avi', 'UCF101_subset/train/Archery/v_Archery_g13_c03.avi', 'UCF101_subset/train/Archery/v_Archery_g05_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g04_c01.avi', 'UCF101_subset/train/Archery/v_Archery_g06_c03.avi', 'UCF101_subset/train/Archery/v_Archery_g22_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g24_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g25_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g04_c03.avi', 'UCF101_subset/train/Archery/v_Archery_g20_c05.avi', 'UCF101_subset/train/Archery/v_Archery_g07_c05.avi', 'UCF101_subset/train/Archery/v_Archery_g06_c01.avi', 'UCF101_subset/train/Archery/v_Archery_g19_c03.avi', 'UCF101_subset/train/Archery/v_Archery_g08_c01.avi', 'UCF101_subset/train/Archery/v_Archery_g10_c05.avi', 'UCF101_subset/train/Archery/v_Archery_g01_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g06_c05.avi', 'UCF101_subset/train/Archery/v_Archery_g13_c07.avi', 'UCF101_subset/train/Archery/v_Archery_g03_c02.avi', 'UCF101_subset/train/Archery/v_Archery_g14_c04.avi', 'UCF101_subset/train/Archery/v_Archery_g07_c01.avi', 'UCF101_subset/train/Archery/v_Archery_g02_c01.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g01_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g23_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g01_c04.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g15_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g05_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g21_c01.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g03_c01.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g18_c04.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g17_c01.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g02_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g03_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g17_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g13_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g04_c01.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g24_c05.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g11_c04.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g21_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g19_c04.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g02_c01.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g19_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c06.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g12_c04.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g08_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g09_c04.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g18_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g04_c05.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g06_c03.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g07_c02.avi', 'UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g07_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g04_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g23_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g03_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g25_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g13_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g19_c05.avi', 'UCF101_subset/train/Basketball/v_Basketball_g23_c06.avi', 'UCF101_subset/train/Basketball/v_Basketball_g21_c03.avi', 'UCF101_subset/train/Basketball/v_Basketball_g04_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g01_c07.avi', 'UCF101_subset/train/Basketball/v_Basketball_g03_c06.avi', 'UCF101_subset/train/Basketball/v_Basketball_g01_c01.avi', 'UCF101_subset/train/Basketball/v_Basketball_g13_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g15_c07.avi', 'UCF101_subset/train/Basketball/v_Basketball_g11_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g16_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g15_c01.avi', 'UCF101_subset/train/Basketball/v_Basketball_g09_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g01_c05.avi', 'UCF101_subset/train/Basketball/v_Basketball_g01_c03.avi', 'UCF101_subset/train/Basketball/v_Basketball_g09_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g14_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g19_c07.avi', 'UCF101_subset/train/Basketball/v_Basketball_g08_c04.avi', 'UCF101_subset/train/Basketball/v_Basketball_g12_c01.avi', 'UCF101_subset/train/Basketball/v_Basketball_g14_c02.avi', 'UCF101_subset/train/Basketball/v_Basketball_g22_c05.avi', 'UCF101_subset/train/Basketball/v_Basketball_g16_c06.avi', 'UCF101_subset/train/Basketball/v_Basketball_g17_c04.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g08_c07.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g08_c01.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g25_c01.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g18_c04.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g23_c03.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g05_c06.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g13_c03.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g13_c07.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g04_c01.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g18_c06.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g02_c02.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g14_c04.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g23_c01.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g06_c07.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g03_c02.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g13_c05.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g08_c05.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g01_c02.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g17_c05.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g21_c02.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g06_c05.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g19_c05.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g03_c04.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g16_c03.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g02_c04.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g14_c02.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g01_c04.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g10_c03.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g19_c03.avi', 'UCF101_subset/train/BaseballPitch/v_BaseballPitch_g15_c06.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g10_c03.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g15_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g03_c04.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g04_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g21_c03.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g13_c01.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g13_c07.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g13_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g04_c01.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g23_c02.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g06_c01.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g20_c04.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g14_c02.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g07_c02.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g19_c06.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g07_c04.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g08_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g06_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g16_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g22_c01.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g24_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g18_c03.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g01_c05.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g03_c06.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g21_c01.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g02_c03.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g02_c01.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g12_c06.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g03_c02.avi', 'UCF101_subset/train/BenchPress/v_BenchPress_g22_c03.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c03.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c03.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c03.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c02.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c07.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c01.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c02.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c05.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c01.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c02.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c06.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c01.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c05.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c05.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c04.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c05.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c03.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c01.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c03.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c04.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c07.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c05.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c04.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c02.avi', 'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c06.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g07_c03.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g13_c04.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g19_c03.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g25_c07.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g19_c05.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g06_c01.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g03_c05.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g02_c06.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g03_c03.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g14_c05.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g07_c05.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g04_c04.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g04_c02.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g16_c06.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g21_c02.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g02_c02.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g25_c05.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g09_c07.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g24_c01.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g10_c02.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g12_c02.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g03_c01.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g07_c07.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g01_c01.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g15_c05.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g21_c06.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g08_c02.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g10_c06.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g09_c03.avi', 'UCF101_subset/train/BandMarching/v_BandMarching_g18_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g18_c02.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g08_c04.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g06_c04.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g10_c04.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c07.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g24_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c05.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c05.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g07_c06.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g16_c05.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g17_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g01_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g05_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g24_c05.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c05.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g08_c02.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g14_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g07_c02.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g14_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g17_c05.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g23_c01.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g12_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c03.avi', 'UCF101_subset/train/BabyCrawling/v_BabyCrawling_g25_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g19_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g01_c02.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g15_c05.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g16_c06.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g07_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g13_c02.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g08_c05.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g25_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g04_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g23_c05.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g22_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g06_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g17_c02.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g01_c04.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g16_c04.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g02_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g24_c04.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g21_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g10_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g16_c02.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g11_c04.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g21_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g15_c01.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g09_c02.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g20_c05.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g03_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g18_c05.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g15_c03.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g07_c05.avi', 'UCF101_subset/train/BasketballDunk/v_BasketballDunk_g23_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g21_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g12_c04.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g10_c04.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g10_c02.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g02_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g23_c04.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g17_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g09_c02.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g24_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g04_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g03_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g08_c04.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g19_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g07_c02.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g15_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g21_c05.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g19_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g25_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g08_c02.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g14_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g04_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g18_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g03_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g05_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g05_c01.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g14_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g01_c03.avi', 'UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c05.avi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qsUgeimGzQ97",
        "outputId": "53d4d387-3c99-4d0e-d703-599ff073e70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
          ]
        }
      ],
      "source": [
        "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
        "label2id = {label: i for i, label in enumerate(class_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(f\"Unique classes: {list(label2id.keys())}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIHwsfPdzQ98"
      },
      "source": [
        "There are 10 unique classes. For each class, there are 30 videos in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twnrA-JxzQ98"
      },
      "source": [
        "## Load a model to fine-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K60IFfRzzQ98"
      },
      "source": [
        "Instantiate a video classification model from a pretrained checkpoint and its associated image processor. The model's encoder comes with pre-trained parameters, and the classification head is randomly initialized. The image processor will come in handy when writing the preprocessing pipeline for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vTDJZfAnzQ98",
        "outputId": "913c58b2-b44d-4df1-d8f5-0e64e329e351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
        "\n",
        "model_ckpt = \"MCG-NJU/videomae-base\"\n",
        "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
        "model = VideoMAEForVideoClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40C0RFD0zQ98"
      },
      "source": [
        "While the model is loading, you might notice the following warning:\n",
        "\n",
        "```bash\n",
        "Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\n",
        "- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "```\n",
        "\n",
        "The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some others (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\n",
        "\n",
        "**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy7b0YAizQ98"
      },
      "source": [
        "## Prepare the datasets for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJK9Io-kzQ98"
      },
      "source": [
        "For preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/). Start by importing the dependencies we need."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorchvideo"
      ],
      "metadata": {
        "id": "cOqUooM-4UFi",
        "outputId": "0a653225-7df8-4fd2-8607-e2a2e95c972f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorchvideo in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (0.1.5.post20221221)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (14.1.0)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oCxOrbJzQ98"
      },
      "source": [
        "For the training dataset transformations, use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, keep the same transformation chain except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorchVideo](https://pytorchvideo.org).  \n",
        "\n",
        "Use the `image_processor` associated with the pre-trained model to obtain the following information:\n",
        "\n",
        "* Image mean and standard deviation with which the video frame pixels will be normalized.\n",
        "* Spatial resolution to which the video frames will be resized.\n",
        "\n",
        "Start by defining some constants."
      ]
    },
    {
      "source": [
        "#!pip install --upgrade torchvision"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8DDQaRN55-GS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0pLdqB-BzQ98"
      },
      "outputs": [],
      "source": [
        "import pytorchvideo.data\n",
        "\n",
        "# from pytorchvideo.transforms import (\n",
        "#     ApplyTransformToKey,\n",
        "#     Normalize,\n",
        "#     RandomShortSideScale,\n",
        "#     RemoveKey,\n",
        "#     ShortSideScale,\n",
        "#     UniformTemporalSubsample,\n",
        "# )\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Lambda,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ApplyTransformToKey:\n",
        "    def __init__(self, key, transform):\n",
        "        self.key = key\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample[self.key] = self.transform(sample[self.key])\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "Qsdg9k3Cj8SX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pSMLYjQkzQ98"
      },
      "outputs": [],
      "source": [
        "mean = image_processor.image_mean\n",
        "std = image_processor.image_std\n",
        "if \"shortest_edge\" in image_processor.size:\n",
        "    height = width = image_processor.size[\"shortest_edge\"]\n",
        "else:\n",
        "    height = image_processor.size[\"height\"]\n",
        "    width = image_processor.size[\"width\"]\n",
        "resize_to = (height, width)\n",
        "\n",
        "num_frames_to_sample = model.config.num_frames\n",
        "sample_rate = 4\n",
        "fps = 30\n",
        "clip_duration = num_frames_to_sample * sample_rate / fps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr15XrvBzQ98"
      },
      "source": [
        "Now, define the dataset-specific transformations and the datasets respectively. Starting with the training set:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorchvideo import transforms\n"
      ],
      "metadata": {
        "id": "onupRXfokoCz",
        "outputId": "3b9fcf66-5708-4d16-fa38-b5227ca6f249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.transforms.functional_tensor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bf87f52a99e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorchvideo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAugMix\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCutMix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixVideo\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrand_augment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pytorchvideo.transforms.augmentations import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_AUGMENTATION_MAX_LEVEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mAugmentTransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmentations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "azZs8LCDzQ98",
        "outputId": "7ad06cc2-332f-4f05-ed59-aae751436c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.transforms.functional_tensor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-31e6aadc89bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorchvideo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m train_transform = Compose(\n\u001b[1;32m      3\u001b[0m     [\n\u001b[1;32m      4\u001b[0m         ApplyTransformToKey(\n\u001b[1;32m      5\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAugMix\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCutMix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixVideo\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrand_augment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pytorchvideo.transforms.augmentations import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_AUGMENTATION_MAX_LEVEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mAugmentTransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmentations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from pytorchvideo import transforms\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        ApplyTransformToKey(\n",
        "            key=\"video\",\n",
        "            transform=Compose(\n",
        "                [\n",
        "                    #UniformTemporalSubsample(num_frames_to_sample),\n",
        "                    Lambda(lambda x: x / 255.0),\n",
        "                    transforms.Normalize(mean, std),\n",
        "                    transforms.RandomShortSideScale(min_size=256, max_size=320),\n",
        "                    RandomCrop(resize_to),\n",
        "                    RandomHorizontalFlip(p=0.5),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = pytorchvideo.data.Ucf101(\n",
        "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
        "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
        "    decode_audio=False,\n",
        "    transform=train_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHW9-1hzQ99"
      },
      "source": [
        "The same sequence of workflow can be applied to the validation and evaluation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wwnawz1zQ99"
      },
      "outputs": [],
      "source": [
        "val_transform = Compose(\n",
        "    [\n",
        "        ApplyTransformToKey(\n",
        "            key=\"video\",\n",
        "            transform=Compose(\n",
        "                [\n",
        "                    UniformTemporalSubsample(num_frames_to_sample),\n",
        "                    Lambda(lambda x: x / 255.0),\n",
        "                    Normalize(mean, std),\n",
        "                    Resize(resize_to),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_dataset = pytorchvideo.data.Ucf101(\n",
        "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
        "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
        "    decode_audio=False,\n",
        "    transform=val_transform,\n",
        ")\n",
        "\n",
        "test_dataset = pytorchvideo.data.Ucf101(\n",
        "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
        "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
        "    decode_audio=False,\n",
        "    transform=val_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHmIA8tdzQ99"
      },
      "source": [
        "**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorchVideo dataset. So, if you want to use a custom dataset not supported off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine.\n",
        "\n",
        "You can access the `num_videos` argument to know the number of videos in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAqklgI2zQ99"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6rViQh1zQ99"
      },
      "source": [
        "## Visualize the preprocessed video for better debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gCk6KBTzQ99"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "def unnormalize_img(img):\n",
        "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
        "    img = (img * std) + mean\n",
        "    img = (img * 255).astype(\"uint8\")\n",
        "    return img.clip(0, 255)\n",
        "\n",
        "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
        "    \"\"\"Prepares a GIF from a video tensor.\n",
        "\n",
        "    The video tensor is expected to have the following shape:\n",
        "    (num_frames, num_channels, height, width).\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    for video_frame in video_tensor:\n",
        "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
        "        frames.append(frame_unnormalized)\n",
        "    kargs = {\"duration\": 0.25}\n",
        "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
        "    return filename\n",
        "\n",
        "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
        "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
        "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
        "    gif_filename = create_gif(video_tensor, gif_name)\n",
        "    return Image(filename=gif_filename)\n",
        "\n",
        "sample_video = next(iter(train_dataset))\n",
        "video_tensor = sample_video[\"video\"]\n",
        "display_gif(video_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSsc-NIZzQ99"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P7iI8JczQ99"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6pkCbnHzQ99"
      },
      "source": [
        "Leverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  ðŸ¤— Transformers for training the model. To instantiate a `Trainer`, you need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on ðŸ¤— Hub.\n",
        "\n",
        "Most of the training arguments are self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in this case, you need the unused features ('video' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1ch3MMIzQ99"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model_name = model_ckpt.split(\"/\")[-1]\n",
        "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
        "num_epochs = 4\n",
        "\n",
        "args = TrainingArguments(\n",
        "    new_model_name,\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYHWbn-zQ99"
      },
      "source": [
        "The dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As such, we must define `max_steps` when instantiating `TrainingArguments`.\n",
        "\n",
        "Next, you need to define a function to compute the metrics from the predictions, which will use the `metric` you'll load now. The only preprocessing you have to do is to take the argmax of our predicted logits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX9ybHIJzQ99"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM0tt0PUzQ99"
      },
      "source": [
        "**A note on evaluation**:\n",
        "\n",
        "In the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial.\n",
        "\n",
        "Also, define a `collate_fn`, which will be used to batch examples together. Each batch consists of 2 keys, namely `pixel_values` and `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmbdkDNYzQ99"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    # permute to (num_frames, num_channels, height, width)\n",
        "    pixel_values = torch.stack(\n",
        "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
        "    )\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNn83IESzQ99"
      },
      "source": [
        "Then you just pass all of this along with the datasets to `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak8rQmjbzQ99"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaxK8wMOzQ-B"
      },
      "source": [
        "You might wonder why you passed along the `image_processor` as a tokenizer when you preprocessed the data already. This is only to make sure the image processor configuration file (stored as JSON) will also be uploaded to the repo on the Hub.\n",
        "\n",
        "Now fine-tune our model by calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JImFFNfHzQ-B"
      },
      "outputs": [],
      "source": [
        "train_results = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVQdDwnqzQ-B"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgvljQkkzQ-B"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eP26el6zQ-B"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiURCKWXzQ-B"
      },
      "source": [
        "Great, now that you have fine-tuned a model, you can use it for inference!\n",
        "\n",
        "Load a video for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzoOrM4yzQ-B"
      },
      "outputs": [],
      "source": [
        "sample_test_video = next(iter(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvIpDVOMzQ-B"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif\" alt=\"Teams playing basketball\"/>\n",
        "</div>\n",
        "\n",
        "The simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). Instantiate a `pipeline` for video classification with your model, and pass your video to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MADStuZUzQ-B"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n",
        "video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUWgOHWozQ-C"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyKTfGOWzQ-C"
      },
      "outputs": [],
      "source": [
        "def run_inference(model, video):\n",
        "    # (num_frames, num_channels, height, width)\n",
        "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
        "    inputs = {\n",
        "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
        "        \"labels\": torch.tensor(\n",
        "            [sample_test_video[\"label\"]]\n",
        "        ),  # this can be skipped if you don't have labels available.\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model = model.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d0T1a_9zQ-C"
      },
      "source": [
        "Now, pass your input to the model and return the `logits`:\n",
        "\n",
        "```\n",
        ">>> logits = run_inference(trained_model, sample_test_video[\"video\"])\n",
        "```\n",
        "\n",
        "Decoding the `logits`, we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkmHHqrFzQ-C"
      },
      "outputs": [],
      "source": [
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "441fc9705f6b4c739d95c57c7d8d0208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7d20c6afd0248b2956f6bbca1955e83",
              "IPY_MODEL_ef52f039117449709dc6c1d020006588",
              "IPY_MODEL_3bf6e549c52f498da472d0788281fd86",
              "IPY_MODEL_84f8c22c7c444ec2948d8fe4894dc689",
              "IPY_MODEL_5f6ff365fcd24d338137aca8679e5aa4"
            ],
            "layout": "IPY_MODEL_251bee1aaed14ceb89bfc0840848fa6e"
          }
        },
        "e7d20c6afd0248b2956f6bbca1955e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6c994e22364c9fa6640baf8688be55",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ec99d6398f414a7ab769e0a663240464",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ef52f039117449709dc6c1d020006588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_83b4f30015284347a82ab9ea6c5ea286",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_018dd2c59eee449ea1eb0bf0c849bf89",
            "value": ""
          }
        },
        "3bf6e549c52f498da472d0788281fd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4e3066af95d74634abd9cd03b6584715",
            "style": "IPY_MODEL_da2decdef76b4773b6ecfe061ad5b54a",
            "value": true
          }
        },
        "84f8c22c7c444ec2948d8fe4894dc689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_10ac724140c34267aa947b97be1f8c15",
            "style": "IPY_MODEL_2236c28ebc8c4aacb8e64b7a199c817e",
            "tooltip": ""
          }
        },
        "5f6ff365fcd24d338137aca8679e5aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2133b7f105f843fd926e4a419fb2a825",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8e01cfe9eb104fcfa1e19121c392db88",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "251bee1aaed14ceb89bfc0840848fa6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1c6c994e22364c9fa6640baf8688be55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec99d6398f414a7ab769e0a663240464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83b4f30015284347a82ab9ea6c5ea286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "018dd2c59eee449ea1eb0bf0c849bf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e3066af95d74634abd9cd03b6584715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2decdef76b4773b6ecfe061ad5b54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10ac724140c34267aa947b97be1f8c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2236c28ebc8c4aacb8e64b7a199c817e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2133b7f105f843fd926e4a419fb2a825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e01cfe9eb104fcfa1e19121c392db88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}